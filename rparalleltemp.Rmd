---
title: "R template for parallel code on machine"
output: html_notebook
---



```{r}
b<-100
lapply(1:3, function(x) b+x)
sapply(1:3, function(x) b+x)
```


makeCluster from the parallel package. Since I am making a cluster locally on my machine I want to leave
some cores free, so I use 2 fewer clusters than I have.  I want to use the the option FORK when I make my cluster. This only works on Mac/Unix systems but it will include all your environmental variables.
```{r}
library(parallel)

# Calculate the number of cores and substract 2
number_cores <- detectCores() - 2
 
```

#### 1.Set up Workers: Fork
I want to create the worker process by forking (makeForkCluster). Forking only works on Mac/Unix systems but is preffered to the default (makePSOCKcluster) which sets up a worker process which listens on a socket for expressions to evaluate. 

Forking is preferred because:
  * all environmental variables are included (no manual export to cluster)
  
  * variables will keep the same addressess (saves memory)

```{r}
my.cluster <- makeCluster(number_cores,type="FORK")
```

## 2. Apply function in parallel to a list or vector
* lapply --> parLapply (list to list ) 
* sapply --> parSapply (list to array)
```{r}

# cluster will only be able 
b<-100

# create a cluster using 6 of my computer cores FORK in environmental variables
my.cluster <- makeCluster(number_cores,type="FORK")

# use parralel version of lapply (list to list)
my.list<-parLapply(my.cluster, 1:5,function(x) x+b)

#use parallel version of sapply (list to array)
my.vector<-parSapply(my.cluster,1:5,function(x) x+b)

# stop cluster and release resources
stopCluster(my.cluster)

print("my.list")
print(my.list)
print("my.vector")
print(my.vector)

```
## parallel looping with foreach

forEach
* additionally needs: library(foreach), library(doParallel)
* Looping construct for executing R code repeatedly
* supports parallel execution
* Note: cluster needs to be registered with registerDoParallel

works like the parallel version of sapply. The difference is foreach allows us to combine sapply type output in a particular way

The .combine argument allows us to define how we want to combine the results from each core.

```{r}
library(foreach)
library(Smisc)
library(doParallel)
library(iterators)
```

```{r}
# multiple ... arguments
the.funct<- function(i,j,k){i+j*k}
foreach(i=1:4, j=4:8,k=9:12) %do% the.funct(i,j,k)
```
```{r}
# multiple ... arguments
the.funct<- function(i,j,k){i+j*k}
timeIt(seq.output<-foreach(i=1:4, j=4:8,k=9:12,.combine=c) %do% the.funct(i,j,k))
print(seq.output)
```

```{r}
timeIt(foreach(i=1:100000*20, j=1:100000*40,k=1:100000,.combine=c) %do% the.funct(i,j,k))
```



```{r}
#set up cluster: only fork on non-windows
my.cluster<-makeCluster(number_cores,type="FORK")
#register mycluster for %dopar%
registerDoParallel(my.cluster)
#run iteration over clusters
timeIt(par.output<-foreach(i=1:4,j=4:8,k=8:12,.combine = c)  %dopar%  the.funct(i,j,k))
# stop cluster and release resources
stopCluster(my.cluster)
print(par.output)
```


```{r}
library(foreach)
library(parallel)
library(Smisc)
library(doParallel)
library(iterators)
```

```{r}
# multiple ... arguments
max.val<-10^4
the.funct<- function(i){sin(i)+cos(i+pi*i)+tan(i+2*pi)*1:5}
timeIt(seq.output<-foreach(i=1:max.val,.combine=c) %do% the.funct(i))
```



```{r}
#Calculate the number of cores and substract 2
number_cores <- detectCores() - 2
#set up cluster: only fork on non-windows
my.cluster<-makeCluster(number_cores,type="FORK")
#register mycluster for %dopar%
registerDoParallel(my.cluster)
the.funct<- function(i){sin(i)+cos(i+pi*i)+tan(i+2*pi)*1:5}
timeIt(par.output<-foreach(i=1:max.val,.combine=c) %dopar% the.funct(i))
# stop cluster and release resources
stopCluster(my.cluster)
print(par.output[3])
```