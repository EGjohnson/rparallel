---
title: "R template for parallel code on machine"
output: html_notebook
---



```{r}
lapply(1:5, function(x) c(x+100,x+200,x+300))
```


makeCluster from the parallel package. Since I am making a cluster locally on my machine I want to leave
some cores free, so I use 2 fewer clusters than I have.  I want to use the the option FORK when I make my cluster. This only works on Mac/Unix systems but it will include all your environmental variables.
```{r}
library(parallel)

# Calculate the number of cores and substract 2
number_cores <- detectCores() - 2
 
```

#### 1.Set up Workers: Fork
I want to create the worker process by forking (makeForkCluster). Forking only works on Mac/Unix systems but is preffered to the default (makePSOCKcluster) which sets up a worker process which listens on a socket for expressions to evaluate. 

Forking is preferred because
  * include all your environmental variables (no manual export to cluster)
  * variables will keep the same addressess (saves memory)

```{}
my.cluster <- makeCluster(number_cores,type="FORK")
```

## 2. Apply function in parallel to a list or vector
* lapply --> parLapply (list to list ) 
* sapply --> parSapply (list to array)
```{r}

# cluster will only be able 
b<-100

# create a cluster using 6 of my computer cores FORK in environmental variables
my.cluster <- makeCluster(number_cores,type="FORK")

# use parralel version of lapply (list to list)
my.list<-parLapply(my.cluster, 1:5,function(x) x+b)

#use parallel version of sapply (list to array)
my.vector<-parSapply(my.cluster,1:5,function(x) x+b)

# stop cluster and release resources
stopCluster(my.cluster)

print("my.list")
print(my.list)
print("my.vector")
print(my.vector)

```
## parallel looping with foreach

forEach
* additionally needs: library(foreach), library(doParallel)
* Looping construct for executing R code repeatedly
* supports parallel execution
* Note: cluster needs to be registered with registerDoParallel

works like the parallel version of sapply. The difference is foreach allows us to combine sapply type output in a particular way

The .combine argument allows us to define how we want to combine the results from each core.

```{r}
library(foreach)
library(Smisc)
library(doParallel)
```

```{r}
# multiple ... arguments
the.funct<- function(i,j,k){i+j*k}
foreach(i=1:4, j=4:8,k=9:12) %do% the.funct(i,j,k)
```


```{r}
timeIt(foreach(i=1:100000*20, j=1:100000*40,k=1:100000,.combine=c) %do% the.funct(i,j,k))
```



```{r}
#set up cluster: only fork on non-windows
my.cluster<-makeCluster(number_cores,type="FORK")
#register mycluster for %dopar%
registerDoParallel(my.cluster)
#run iteration over clusters
timeIt(foreach(i=1:100000*20, j=1:100000*40,k=1:100000,.combine = c)  %dopar%  the.funct(i,j,k))
# stop cluster and release resources
stopCluster(my.cluster)
```



Running jobs in parallel incurs overhead. Only if the jobs you fire at the worker nodes take a significant amount of time does parallelization improve overall performance. When the individual jobs take only milliseconds, the overhead of constantly firing off jobs will deteriorate overall performance. The trick is to divide the work over the nodes in such a way that the jobs are sufficiently long, say at least a few seconds. I used this to great effect running six Fortran models simultaneously, but these individual model runs took hours, almost negating the effect of overhead.

Note that I haven't run your example, but the situation I describe above is often the issue when parallization takes longer than running sequentially.

On each of the worker nodes there is an R process, the overhead consists of sending instructions to each node telling them what to do, and gathering back the results

construct a bit more demanding function